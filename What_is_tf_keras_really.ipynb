{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "What is tf.keras really.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdlxCDeop4DI",
        "colab_type": "text"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/jaredwinick/what_is_tf_keras/blob/master/What_is_tf_keras_really.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/jaredwinick/what_is_tf_keras/blob/master/What_is_tf_keras_really.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQtEyguXHI2e",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "What is `tf.keras` *really*?\n",
        "============================\n",
        "This tutorial has been ported to TensorFlow 2.0 by Jared Winick. **All credit for should be given to Jeremy Howard [fast.ai](https://www.fast.ai) and his collaborators Rachel Thomas and Francisco Ingham for the original post [\"What is `torch.nn` *really*\"](https://pytorch.org/tutorials/beginner/nn_tutorial.html).** Much of the original content has been copied verbatim. I really enjoyed the teaching style of the original and following it while porting to TF 2.0 was very helpful in learning that basics. It should be noted I am no expert in TF so if there are ways to improve the code, please submit a PR! Hopefully this tutorial can be as useful as the original, but for those interested in TensorFlow 2 as opposed to PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvgMRxuXHI2f",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow provides several high-level modules and classes such as [``tf.keras.layers``](https://www.tensorflow.org/api_docs/python/tf/keras/layers),\n",
        "[``tf.keras.optimizers``](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers), and\n",
        "[``tf.data.Dataset``](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) \n",
        "to help you create and train neural networks.\n",
        "In order to fully utilize their power and customize\n",
        "them for your problem, you need to really understand exactly what they're\n",
        "doing. To develop this understanding, we will first train a basic neural net\n",
        "on the MNIST data set without using any features from these modules; we will\n",
        "initially only use the most basic TensorFlow tensor functionality. Then, we will\n",
        "incrementally add one feature from ``tf.keras.layers``, ``tf.keras.optimizers``, or\n",
        "``Dataset`` at a time, showing exactly what each piece does, and how it\n",
        "works to make the code either more concise, or more flexible.\n",
        "\n",
        "**This tutorial assumes you already have TensorFlow installed, and are familiar\n",
        "with the basics of tensor operations.** Using Google Colab is an easy way to start using TensorFlow 2.0 without having to install anything yourself.\n",
        "\n",
        "MNIST data setup\n",
        "----------------\n",
        "\n",
        "We will use the classic [MNIST](http://deeplearning.net/data/mnist/) dataset,\n",
        "which consists of black-and-white images of hand-drawn digits (between 0 and 9).\n",
        "\n",
        "We will use [pathlib](https://docs.python.org/3/library/pathlib.html)\n",
        "for dealing with paths (part of the Python 3 standard library), and will\n",
        "download the dataset using\n",
        "[requests](http://docs.python-requests.org/en/master/). We will only\n",
        "import modules when we use them, so you can see exactly what's being\n",
        "used at each point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtppAP10HI2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "DATA_PATH = Path(\"data\")\n",
        "PATH = DATA_PATH / \"mnist\"\n",
        "\n",
        "PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "URL = \"http://deeplearning.net/data/mnist/\"\n",
        "FILENAME = \"mnist.pkl.gz\"\n",
        "\n",
        "if not (PATH / FILENAME).exists():\n",
        "        content = requests.get(URL + FILENAME).content\n",
        "        (PATH / FILENAME).open(\"wb\").write(content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqHNcDiXHI2j",
        "colab_type": "text"
      },
      "source": [
        "This dataset is in numpy array format, and has been stored using pickle,\n",
        "a python-specific format for serializing data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDbRLZm-HI2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import gzip\n",
        "\n",
        "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
        "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSXiGlYHHI2n",
        "colab_type": "text"
      },
      "source": [
        "Each image is 28 x 28, and is being stored as a flattened row of length\n",
        "784 (=28x28). Let's take a look at one; we need to reshape it to 2d\n",
        "first.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG46yYYbHI2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5eyvt1QHI2n",
        "colab_type": "code",
        "outputId": "26e64b2f-b607-4399-a39f-22106187650e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "\n",
        "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
        "print(x_train.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 784)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpSiR87YHI2q",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow uses ``tf.Tensor``, rather than numpy arrays, so we need to convert out data. We use the function ``tf.constant`` to convert the numpy array into a TensorFlow tensor. See this following for more detail on numpy compatibility https://www.tensorflow.org/tutorials/customization/basics#numpy_compatibility\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-isB3d_AK3CY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1CJn1ROHI2r",
        "colab_type": "code",
        "outputId": "4579ffce-021b-4c16-ea27-acbd7242a099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x_train, y_train, x_valid, y_valid = map(\n",
        "    tf.constant, (x_train, y_train, x_valid, y_valid)\n",
        ")\n",
        "n, c = x_train.shape\n",
        "print(x_train)\n",
        "print(y_train)\n",
        "print(f'Min label: {tf.math.reduce_min(y_train).numpy()}')\n",
        "print(f'Max label: {tf.math.reduce_max(y_train).numpy()}')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]], shape=(50000, 784), dtype=float32)\n",
            "tf.Tensor([5 0 4 ... 8 4 8], shape=(50000,), dtype=int64)\n",
            "Min label: 0\n",
            "Max label: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ9Eb-DgHI2v",
        "colab_type": "text"
      },
      "source": [
        "Neural net from scratch (no tf.keras)\n",
        "---------------------------------------------\n",
        "\n",
        "Let's first create a model using nothing but TensorFlow tensor operations. We're assuming\n",
        "you're already familiar with the basics of neural networks. (If you're not, you can\n",
        "learn them at [course.fast.ai](https://course.fast.ai>)).\n",
        "\n",
        "TensorFlow provides methods to create random or zero-filled tensors, which we will\n",
        "use to create our weights and bias for a simple linear model. If you are comparing this post with the original using PyTorch, you can see some differences here with respect to how the frameworks are enabling gradients to be calculated. In TensorFlow you don't specify that a tensor requires gradients calculated on the object itself, but by using those tensors in the context of a `tf.GradientTape` as seen in a few paragraphs.\n",
        "\n",
        "A similar exercise can be see in the TensorFlow documentation [Custom training](https://www.tensorflow.org/tutorials/customization/custom_training)\n",
        "\n",
        "*Note*: We are initializing the weights here with [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) by multiplying with 1/sqrt(n).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltuWjkEGHI2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "weights = tf.Variable(tf.random.normal((784, 10)) / math.sqrt(784))\n",
        "bias = tf.Variable(tf.zeros(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4SmYspQHI2y",
        "colab_type": "text"
      },
      "source": [
        "Thanks to TensorFlow's ability to calculate gradients automatically, we can\n",
        "use any standard Python function (or callable object) as a model! So\n",
        "let's just write a plain matrix multiplication and broadcasted addition\n",
        "to create a simple linear model. We also need an activation function, so\n",
        "we'll write `log_softmax` and use it. Remember: although TensorFlow\n",
        "provides lots of pre-written loss functions, activation functions, and\n",
        "so forth, you can easily write your own using plain python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrmuRdAtHI2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_softmax(x):\n",
        "  return x - tf.math.log(tf.math.reduce_sum(tf.math.exp(x), -1, keepdims=True))\n",
        "\n",
        "def model(xb):\n",
        "  return log_softmax(xb @ weights + bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qDp0hcxHI21",
        "colab_type": "text"
      },
      "source": [
        "In the above, the ``@`` stands for the dot product operation. We will call\n",
        "our function on one batch of data (in this case, 64 images).  This is\n",
        "one *forward pass*.  Note that our predictions won't be any better than\n",
        "random at this stage, since we start with random weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY0f-CmwHI22",
        "colab_type": "code",
        "outputId": "7fcd4e65-966b-4fbe-a0cf-d17c3f633f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "bs = 64  # batch size\n",
        "\n",
        "xb = x_train[0:bs]  # a mini-batch from x\n",
        "preds = model(xb)  # predictions\n",
        "print(preds[0], preds.shape)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[-2.574955  -2.6288726 -2.3458853 -2.079679  -2.2557921 -2.4567945\n",
            " -2.7491755 -2.2003245 -1.7462468 -2.3938751], shape=(10,), dtype=float32) (64, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPnUCCs_HI25",
        "colab_type": "text"
      },
      "source": [
        "Let's implement negative log-likelihood to use as the loss function\n",
        "(again, we can just use standard Python):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swlqlM9QHI26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nll(input, target):\n",
        "    indices = tf.stack([tf.range(input.shape[0], dtype=tf.int64), target], axis=1)\n",
        "    return tf.math.reduce_mean(-tf.gather_nd(input, indices))\n",
        "\n",
        "loss_func = nll"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p38D0BpMHI2-",
        "colab_type": "text"
      },
      "source": [
        "Let's check our loss with our random model, so we can see if we improve\n",
        "after a backprop pass later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSR-IaHMHI2_",
        "colab_type": "code",
        "outputId": "cc4a9b42-fced-4274-b206-87072817f20d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "yb = y_train[0:bs]\n",
        "print(loss_func(preds, yb))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.375083, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r7nzjaqHI3C",
        "colab_type": "text"
      },
      "source": [
        "Let's also implement a function to calculate the accuracy of our model.\n",
        "For each prediction, if the index with the largest value matches the\n",
        "target value, then the prediction was correct.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-oZaElUHI3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(out, yb):\n",
        "    preds = tf.math.argmax(out, axis=1)\n",
        "    return tf.math.reduce_mean(tf.dtypes.cast(preds == yb, tf.float16))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRkuwe9fHI3F",
        "colab_type": "text"
      },
      "source": [
        "Let's check the accuracy of our random model, so we can see if our\n",
        "accuracy improves as our loss improves.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VqzcytwHI3G",
        "colab_type": "code",
        "outputId": "d95f33c4-0388-49c1-d3c3-faa371c74f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(accuracy(preds, yb))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.04688, shape=(), dtype=float16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyr5-d8gHI3J",
        "colab_type": "text"
      },
      "source": [
        "We can now run a training loop.  For each iteration, we will:\n",
        "\n",
        "- select a mini-batch of data (of size ``bs``)\n",
        "- Under a `tf.GradientTape` context\n",
        "  - use the model to make predictions\n",
        "  - calculate the loss\n",
        "- Compute the gradient of operations recorded in the context of this tape.\n",
        "\n",
        "We now use these gradients to update the weights and bias. \n",
        "\n",
        "Documentation can be seen at \n",
        "[``tf.GradientTape``](https://www.tensorflow.org/api_docs/python/tf/GradientTape). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGzyvW9VHI3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.2  # learning rate\n",
        "epochs = 2  # how many epochs to train for\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range((n - 1) // bs + 1):\n",
        "        start_i = i * bs\n",
        "        end_i = start_i + bs\n",
        "        xb = x_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "        with tf.GradientTape() as t:\n",
        "          pred = model(xb)\n",
        "          loss = loss_func(pred, yb)\n",
        "\n",
        "        dW, dB = t.gradient(loss, [weights, bias])\n",
        "        weights.assign_sub(lr * dW)\n",
        "        bias.assign_sub(lr * dB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huDXGV_yHI3R",
        "colab_type": "text"
      },
      "source": [
        "That's it: we've created and trained a minimal neural network (in this case, a\n",
        "logistic regression, since we have no hidden layers) entirely from scratch!\n",
        "\n",
        "Let's check the loss and accuracy and compare those to what we got\n",
        "earlier. We expect that the loss will have decreased and accuracy to\n",
        "have increased, and they have.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tL1kwiWHI3R",
        "colab_type": "code",
        "outputId": "d7f432ab-1934-49c8-fcf0-d4ba1f8d07ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.24429947, shape=(), dtype=float32) tf.Tensor(0.9375, shape=(), dtype=float16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmunBi-FHI3U",
        "colab_type": "text"
      },
      "source": [
        "Using tf.keras\n",
        "------------------------------\n",
        "\n",
        "We will now refactor our code, so that it does the same thing as before, only\n",
        "we'll start taking advantage of TensorFlows's ``tf.keras`` classes to make it more concise\n",
        "and flexible. At each step from here, we should be making our code one or more\n",
        "of: shorter, more understandable, and/or more flexible. It does seem like we the ``tf.nn`` module might provide classes that are more directly inline with the original post's use of ``torch.nn`` but I have chosen to jump right to using ``tf.keras`` as it is the recommended higher level API.\n",
        "\n",
        "The first and easiest step is to make our code shorter by replacing our\n",
        "hand-written activation and loss functions with those from ``tf.keras``.\n",
        "\n",
        "TensorFlow provides a single function ``tf.keras.losses.SparseCategoricalCrossentropy`` that combines\n",
        "a softmax activation with a loss function. Note that we use ``from_logit=True`` here because we are not passing in a probability distribution. See https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHF-7zW-HI3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def model(xb):\n",
        "    return xb @ weights + bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UlJbAu9HI3W",
        "colab_type": "text"
      },
      "source": [
        "Note that we no longer call ``log_softmax`` in the ``model`` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7oYHMMPHI3X",
        "colab_type": "code",
        "outputId": "01836849-89c6-4f59-87c2-4ddddfc24a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(loss_func(yb, model(xb)), accuracy(model(xb), yb))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.24429946, shape=(), dtype=float32) tf.Tensor(0.9375, shape=(), dtype=float16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st07ywgNHI3a",
        "colab_type": "text"
      },
      "source": [
        "Refactor using tf.keras.Model\n",
        "-----------------------------\n",
        "Next up, we'll use ``tf.keras.Model`` for a clearer and more\n",
        "concise training loop. We subclass ``tf.keras.Model`` (which itself is a class and\n",
        "able to keep track of state).  In this case, we want to create a class that\n",
        "holds our weights, bias, and method for the forward step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXcay3O1HI3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mnist_Logistic(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "      super(Mnist_Logistic, self).__init__()\n",
        "      self.w = tf.Variable(tf.random.normal((784, 10)) / math.sqrt(784))\n",
        "      self.b = tf.Variable(tf.zeros(10))\n",
        "\n",
        "    def call(self, xb):\n",
        "        return xb @ self.w + self.b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOA1vNbmHI3h",
        "colab_type": "text"
      },
      "source": [
        "Since we're now using an object instead of just using a function, we\n",
        "first have to instantiate our model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T63G-9haHI3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Mnist_Logistic()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqU-qnZmHI3j",
        "colab_type": "text"
      },
      "source": [
        "Now we can calculate the loss in the same way as before. Note that\n",
        "``tf.keras.Model`` objects are used as if they are functions (i.e they are\n",
        "*callable*), but behind the scenes TensorFlow will call our ``call``\n",
        "method automatically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GthVlmPcHI3k",
        "colab_type": "code",
        "outputId": "81c17e62-c053-48d3-f3ca-5bd120202773",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(loss_func(yb, model(xb)))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.5179658, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xtsNGbvHI3m",
        "colab_type": "text"
      },
      "source": [
        "Previously for our training loop we had to update the values for each parameter\n",
        "by name, like this:\n",
        "\n",
        "```\n",
        "weights.assign_sub(lr * dW)\n",
        "bias.assign_sub(lr * dB)\n",
        "```\n",
        "\n",
        "Now we can take advantage of ``model.trainable_variables`` to make those steps more concise\n",
        "and less prone to the error of forgetting some of our parameters, particularly\n",
        "if we had a more complicated model.\n",
        "\n",
        "We'll wrap our little training loop in a ``fit`` function so we can run it\n",
        "again later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgHrLzYnHI3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit():\n",
        "  for epoch in range(epochs):\n",
        "    for i in range((n - 1) // bs + 1):\n",
        "      start_i = i * bs\n",
        "      end_i = start_i + bs\n",
        "      xb = x_train[start_i:end_i]\n",
        "      yb = y_train[start_i:end_i]\n",
        "      with tf.GradientTape() as t:\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(yb, pred)\n",
        "\n",
        "      gradients = t.gradient(loss, model.trainable_variables)\n",
        "      for variable, grad in zip(model.trainable_variables, gradients):\n",
        "        variable.assign_sub(lr * grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqAjj0pkK_ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbDo0LtWHI3q",
        "colab_type": "text"
      },
      "source": [
        "Let's double-check that our loss has gone down:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv08guPZHI3r",
        "colab_type": "code",
        "outputId": "792dd62a-4ac7-4c75-89e4-57ea2f0b374f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(loss_func(yb, model(xb)), accuracy(model(xb), yb))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.2520585, shape=(), dtype=float32) tf.Tensor(0.9375, shape=(), dtype=float16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmMQ4w3zHI3t",
        "colab_type": "text"
      },
      "source": [
        "Refactor using tf.keras.layers.Dense\n",
        "-------------------------\n",
        "\n",
        "We continue to refactor our code.  Instead of manually defining and\n",
        "initializing ``self.weights`` and ``self.bias``, and calculating ``xb  @\n",
        "self.weights + self.bias``, we will instead use the TensorFlow class\n",
        "[``tf.keras.layers.Dense``](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) for a\n",
        "linear layer, which does all that for us. TensorFlow has many types of\n",
        "predefined layers that can greatly simplify our code, and often makes it\n",
        "faster too.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBpagh5GHI3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mnist_Logistic(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "      super(Mnist_Logistic, self).__init__()\n",
        "      self.linear = tf.keras.layers.Dense(10, input_shape=(None,784))\n",
        "\n",
        "    def call(self, xb):\n",
        "        return self.linear(xb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBlTtad7HI3x",
        "colab_type": "text"
      },
      "source": [
        "We instantiate our model and calculate the loss in the same way as before:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKQObp6UHI3z",
        "colab_type": "code",
        "outputId": "9358d25d-e25c-45a7-c865-8e43c030ba16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = Mnist_Logistic()\n",
        "print(loss_func(yb, model(xb)))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.5036383, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPI3Zhe0HI31",
        "colab_type": "text"
      },
      "source": [
        "We are still able to use our same ``fit`` method as before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CRkN1n7HI32",
        "colab_type": "code",
        "outputId": "9d460f74-ec40-488d-e447-bdb8ac512c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fit()\n",
        "\n",
        "print(loss_func(yb, model(xb)))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.25161958, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxR9uCnqHI34",
        "colab_type": "text"
      },
      "source": [
        "Refactor using tf.keras.optimizers\n",
        "------------------------------\n",
        "\n",
        "TensorFlow also has a package with various optimization algorithms, [``tf.keras.optimizers``](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).\n",
        "We can use the ``apply_gradients`` method from our optimizer to take a forward step, instead\n",
        "of manually updating each parameter.\n",
        "\n",
        "This will let us replace our previous manually coded optimization step:\n",
        "\n",
        "```\n",
        "  for variable, grad in zip(model.trainable_variables, gradients):\n",
        "    variable.assign_sub(lr * grad)\n",
        "```\n",
        "\n",
        "and instead use just:\n",
        "```\n",
        "  opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGD4aPkBHI37",
        "colab_type": "text"
      },
      "source": [
        "We'll define a little function to create our model and optimizer so we\n",
        "can reuse it in the future.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcgVcBxAHI38",
        "colab_type": "code",
        "outputId": "0b9bf90f-4071-4283-84f4-820ec582cc5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def get_model():\n",
        "    model = Mnist_Logistic()\n",
        "    return model, tf.keras.optimizers.SGD(lr=lr)\n",
        "\n",
        "model, opt = get_model()\n",
        "print(loss_func(yb, model(xb)))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range((n - 1) // bs + 1):\n",
        "        start_i = i * bs\n",
        "        end_i = start_i + bs\n",
        "        xb = x_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "\n",
        "        with tf.GradientTape() as t:\n",
        "          pred = model(xb)\n",
        "          loss = loss_func(yb, pred)\n",
        "\n",
        "        gradients = t.gradient(loss, model.trainable_variables)\n",
        "        opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "print(loss_func(yb, model(xb)))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.5305252, shape=(), dtype=float32)\n",
            "tf.Tensor(0.2404473, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3U3Ke5iHI3-",
        "colab_type": "text"
      },
      "source": [
        "Refactor using Dataset\n",
        "------------------------------\n",
        "\n",
        "TensorFlow provides the [``tf.data.Dataset``](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) class as an abstraction over the data needed for machine learning pipelines. A simple way to construct a ``Dataset`` is from existing tensors. The ``Dataset`` can hold the input as well as labels, and also provides an easy way to iterate over batches. For much more info, please see https://www.tensorflow.org/guide/data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGSAQjKQHI4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PxkPFnpHI4E",
        "colab_type": "text"
      },
      "source": [
        "Previously, we had to iterate through minibatches of x and y values separately:\n",
        "```\n",
        "  for i in range((n - 1) // bs + 1):\n",
        "    start_i = i * bs\n",
        "    end_i = start_i + bs\n",
        "    xb = x_train[start_i:end_i]\n",
        "    yb = y_train[start_i:end_i]\n",
        "```\n",
        "\n",
        "Now, we can do this in a single line:\n",
        "```\n",
        "  for xb, yb in train_ds.batch(bs):\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMWm_Mf1HI4E",
        "colab_type": "code",
        "outputId": "1ec735bd-ded2-4c57-f47d-d3a32af4e8de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for xb, yb in train_ds.batch(bs):\n",
        "        with tf.GradientTape() as t:\n",
        "          pred = model(xb)\n",
        "          loss = loss_func(yb, pred)\n",
        "\n",
        "        gradients = t.gradient(loss, model.trainable_variables)\n",
        "        opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "print(loss_func(yb, model(xb)))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.24117279, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NBlWPonHI4K",
        "colab_type": "text"
      },
      "source": [
        "Thanks to TensorFlows's [``tf.keras.Model``](https://www.tensorflow.org/api_docs/python/tf/keras/Model), [``tf.keras.optimizers``](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers), and [``tf.data.Dataset``](https://www.tensorflow.org/api_docs/python/tf/data/Dataset),\n",
        "our training loop is now dramatically smaller and easier to understand. Let's\n",
        "now try to add the basic features necessary to create effecive models in practice.\n",
        "\n",
        "Add validation\n",
        "-----------------------\n",
        "\n",
        "In section 1, we were just trying to get a reasonable training loop set up for\n",
        "use on our training data.  In reality, you **always** should also have\n",
        "a [validation set](https://www.fast.ai/2017/11/13/validation-sets/), in order\n",
        "to identify if you are overfitting.\n",
        "\n",
        "Shuffling the training data is\n",
        "[important](https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks)\n",
        "to prevent correlation between batches and overfitting. On the other hand, the\n",
        "validation loss will be identical whether we shuffle the validation set or not.\n",
        "Since shuffling takes extra time, it makes no sense to shuffle the validation data.\n",
        "\n",
        "We'll use a batch size for the validation set that is twice as large as\n",
        "that for the training set. This is because the validation set does not\n",
        "need backpropagation and thus takes less memory (it doesn't need to\n",
        "store the gradients). We take advantage of this to use a larger batch\n",
        "size and compute the loss more quickly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPF-AtBHHI4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SHUFFLE_BUFFER_SIZE = 100\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(SHUFFLE_BUFFER_SIZE).batch(bs)\n",
        "validation_ds = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(bs * 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WdMAjmKHI4O",
        "colab_type": "text"
      },
      "source": [
        "We will calculate and print the training and validation loss and accuracy at the end of each epoch. To do this we will leverage the [``tf.keras.metrics``](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl4JvovZHI4P",
        "colab_type": "code",
        "outputId": "6ddf93e7-a266-4b22-9b5f-5c9bc9ddf1f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "validation_loss = tf.keras.metrics.Mean(name='validation_loss')\n",
        "validation_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='validation_accuracy')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for xb, yb in train_ds:\n",
        "    with tf.GradientTape() as t:\n",
        "      pred = model(xb)\n",
        "      loss = loss_func(yb, pred)\n",
        "\n",
        "      gradients = t.gradient(loss, model.trainable_variables)\n",
        "      opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "      train_loss(loss)\n",
        "      train_accuracy(yb, pred)\n",
        "\n",
        "  for xb, yb in validation_ds:\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(yb, pred)\n",
        "\n",
        "    validation_loss(loss)\n",
        "    validation_accuracy(yb, pred)  \n",
        "\n",
        "  # From https://www.tensorflow.org/tutorials/quickstart/advanced\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Valid Loss: {}, Valiid Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        validation_loss.result(),\n",
        "                        validation_accuracy.result()*100))\n",
        "\n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  validation_loss.reset_states()\n",
        "  validation_accuracy.reset_states()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.4367533028125763, Accuracy: 88.052001953125, Valid Loss: 0.34305909276008606, Valiid Accuracy: 90.1500015258789\n",
            "Epoch 2, Loss: 0.3235304057598114, Accuracy: 90.90599822998047, Valid Loss: 0.29940706491470337, Valiid Accuracy: 91.6199951171875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5FZ7xlVHI4R",
        "colab_type": "text"
      },
      "source": [
        "Create fit() and get_data()\n",
        "----------------------------------\n",
        "\n",
        "We'll now do a little refactoring of our own. Since we go through a similar\n",
        "process twice of calculating the loss for both the training set and the\n",
        "validation set, let's make that into its own function, ``loss_batch``, which\n",
        "computes the loss for one batch.\n",
        "\n",
        "We pass an optimizer in for the training set, and use it to perform\n",
        "backprop.  For the validation set, we don't pass an optimizer, so the\n",
        "method doesn't perform backprop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x2_CIQgHI4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_batch(model, loss_func, xb, yb, metric_loss, metric_accuracy, opt=None):\n",
        "  with tf.GradientTape() as t:\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(yb, pred)\n",
        "\n",
        "  if opt is not None:\n",
        "    gradients = t.gradient(loss, model.trainable_variables)\n",
        "    opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  metric_loss(loss)\n",
        "  metric_accuracy(yb, pred)\n",
        "  return loss, len(xb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zdRY4UXHI4Z",
        "colab_type": "text"
      },
      "source": [
        "``fit`` runs the necessary operations to train our model and compute the\n",
        "training and validation losses for each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4huU50gBHI4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(epochs, model, loss_func, opt, train_ds, valid_ds):\n",
        "\n",
        "  train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "  validation_loss = tf.keras.metrics.Mean(name='validation_loss')\n",
        "  validation_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='validation_accuracy')\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for xb, yb in train_ds:\n",
        "      loss_batch(model, loss_func, xb, yb, train_loss, train_accuracy, opt)\n",
        "\n",
        "    for xb, yb in valid_ds:\n",
        "      loss_batch(model, loss_func, xb, yb, validation_loss, validation_accuracy)\n",
        "       \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Valid Loss: {}, Valiid Accuracy: {}'\n",
        "    print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        validation_loss.result(),\n",
        "                        validation_accuracy.result()*100))\n",
        "\n",
        "    # Reset the metrics for the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    validation_loss.reset_states()\n",
        "    validation_accuracy.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0no_UNXeHI4e",
        "colab_type": "text"
      },
      "source": [
        "Now, our whole process of fitting the model can be run in 2 lines of code:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3nZPVMhHI4e",
        "colab_type": "code",
        "outputId": "664cf336-18b0-414e-8f80-9fa82d90855a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model, opt = get_model()\n",
        "fit(epochs, model, loss_func, opt, train_ds, validation_ds)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.4374210238456726, Accuracy: 88.06600189208984, Valid Loss: 0.3246917128562927, Valiid Accuracy: 90.66999816894531\n",
            "Epoch 2, Loss: 0.32279902696609497, Accuracy: 90.93800354003906, Valid Loss: 0.2960929870605469, Valiid Accuracy: 91.62999725341797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbnEd5qnHI4g",
        "colab_type": "text"
      },
      "source": [
        "You can use these basic 2 lines of code to train a wide variety of models.\n",
        "Let's see if we can use them to train a convolutional neural network (CNN)!\n",
        "\n",
        "Switch to CNN\n",
        "-------------\n",
        "\n",
        "We are now going to build our neural network with a convolutional layer.\n",
        "Because none of the functions in the previous section assume anything about\n",
        "the model form, we'll be able to use them to train a CNN without any modification.\n",
        "\n",
        "We will use TensorFlow's predefined\n",
        "[``tf.keras.layers.Conv2D``](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) class\n",
        "as our convolutional layer. We define a CNN with 1 convolutional layers just like the architecture here https://www.tensorflow.org/tutorials/quickstart/advanced\n",
        "\n",
        "This time when creating the ``SparseCategoricalCrossentropy`` use the default of ``from_logits=False`` because we are doing a softmax in the last layer of the model, so it is already outputing a probability distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbf5Zt6svHNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mnist_CNN(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Mnist_CNN, self).__init__()\n",
        "    self.reshape1 = tf.keras.layers.Reshape((28,28,1), input_shape=((784,)))\n",
        "    self.conv1 = tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation='relu')\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.d1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "    self.d2 = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self, xb):\n",
        "    xb = self.reshape1(xb)\n",
        "    xb = self.conv1(xb)\n",
        "    xb = self.flatten(xb)\n",
        "    xb = self.d1(xb)\n",
        "    return self.d2(xb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjwdZUFX68HU",
        "colab_type": "code",
        "outputId": "12b4083a-853f-42f1-961d-50188c68de37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model = Mnist_CNN()\n",
        "opt = tf.keras.optimizers.Adam()\n",
        "loss_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_ds, validation_ds)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.23629596829414368, Accuracy: 93.24800109863281, Valid Loss: 0.0928812175989151, Valiid Accuracy: 97.50999450683594\n",
            "Epoch 2, Loss: 0.07652167230844498, Accuracy: 97.6719970703125, Valid Loss: 0.06858094781637192, Valiid Accuracy: 98.02999877929688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkGvsudaHI4n",
        "colab_type": "text"
      },
      "source": [
        "tf.keras.Sequential\n",
        "------------------------\n",
        "\n",
        "``tf.keras`` has another handy class we can use to simply our code\n",
        "[``tf.keras.Sequential``](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential).\n",
        "A ``Sequential`` object runs each of the modules contained within it, in a\n",
        "sequential manner. This is a simpler way of writing our neural network.\n",
        "\n",
        "The model created with ``Sequential`` is simply:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paasWP_19lzd",
        "colab_type": "code",
        "outputId": "cb638b2f-ccfb-4e30-e8b3-f7b77e5147e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Reshape((28,28,1), input_shape=((784,))),\n",
        "    tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation='relu'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')])\n",
        "\n",
        "opt = tf.keras.optimizers.Adam()\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_ds, validation_ds)\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.2498997449874878, Accuracy: 92.91999816894531, Valid Loss: 0.10154058784246445, Valiid Accuracy: 97.1500015258789\n",
            "Epoch 2, Loss: 0.08210417628288269, Accuracy: 97.53400421142578, Valid Loss: 0.07364194095134735, Valiid Accuracy: 97.79999542236328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvi0LjIzIZnt",
        "colab_type": "text"
      },
      "source": [
        "Using the built-in ``fit``\n",
        "--------------------------\n",
        "``tf.keras.Model`` has built-in functions for training and evaluating models so that we don't have to write our own like we did above. The documention at https://www.tensorflow.org/api_docs/python/tf/keras/Model provides detail on the ``compile``, ``fit``, and ``evaluate`` functions used below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw_8kiDr-OfU",
        "colab_type": "code",
        "outputId": "065fbcfd-4f61-4637-d8ca-f19ef4a04a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Reshape((28,28,1), input_shape=((784,))),\n",
        "    tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation='relu'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_ds, epochs=2)\n",
        "model.evaluate(validation_ds)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.2514 - accuracy: 0.9269\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.0825 - accuracy: 0.9752\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 0.0703 - accuracy: 0.9779\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07032434737510225, 0.9779]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITW8Ddo_HI43",
        "colab_type": "text"
      },
      "source": [
        "Using your GPU\n",
        "---------------\n",
        "\n",
        "If you're lucky enough to have access to a CUDA-capable GPU (as you would if you are running this in Google Colab) you can\n",
        "use it to speed up your code. First check that your GPU is working in\n",
        "TensorFlow:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsl1utuiHI44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd40e972-42c4-4947-ce14-4a2c21b97e3d"
      },
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr3WlEGtHI45",
        "colab_type": "text"
      },
      "source": [
        "Luckily TensorFlow will automatically use a GPU if one is present. If you would like to have fine-grained control of how your operations are run on CPU and GPU devices, see https://www.tensorflow.org/guide/gpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q8cRwY6HI5C",
        "colab_type": "text"
      },
      "source": [
        "Closing thoughts\n",
        "-----------------\n",
        "\n",
        "Again, all credit for this tutorial should really go to the originial authors of [\"What is ``torch.nn`` *really*?\"](https://pytorch.org/tutorials/beginner/nn_tutorial.html) Jeremy Howard of [fast.ai](https://fast.ai) with Rachel Thomas and Francisco Ingham. I have really enjoyed the teaching style that Jeremy and Rachel use in their fast.ai courses, and the original tutorial felt very similar to fast.ai material. If you like that teaching style as well, I would highly recommend walking through with the original in PyTorch, or this version with TensorFlow, to get a better feel for what is going on inside these frameworks.\n",
        "\n",
        "From my personal perspective as an engineer, the high-level APIs available in TensorFlow (or PyTorch) are making it easier and easier to apply deep learning. It is an exciting time to see so much progress in lowering the barrier to entry and improving the tools engineers/scientists/researchers have to solve problems."
      ]
    }
  ]
}